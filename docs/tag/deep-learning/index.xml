<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning | Marc B</title>
    <link>https://marcb.pro/tag/deep-learning/</link>
      <atom:link href="https://marcb.pro/tag/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Deep Learning</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Mon, 01 Nov 2021 00:00:00 +0100</lastBuildDate>
    <image>
      <url>https://marcb.pro/media/icon_hu7278467068447599620.png</url>
      <title>Deep Learning</title>
      <link>https://marcb.pro/tag/deep-learning/</link>
    </image>
    
    <item>
      <title>Learning Correspondences For Relative Pose Estimation</title>
      <link>https://marcb.pro/project/gr/</link>
      <pubDate>Mon, 01 Nov 2021 00:00:00 +0100</pubDate>
      <guid>https://marcb.pro/project/gr/</guid>
      <description>&lt;p&gt;We present an end-to-end learnable, differentiable method for pairwise relative pose registration of RGB-D frames. Our method is robust to big camera motions thanks to a self-supervised weighting of the predicted correspondences between the frames. Given a pair of frames, our method estimates matches of points and their visibility score. A self-supervised model predicts a confidence weight for visible matches. Finally, visible matches and their weight are fed into a differentiable weighted Procrustes aligner which estimates the rigid transformation between the input frames.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-pipeline-of-our-method-given-a-pair-of-rgb-d-images-i1-d1-i2-d2-we-estimate-the-relative-pose-between-these-frames-as-r-in-so3-and-t-in-r3--first-i1-i2-are-fed-into-the-correspondence-and-visibility-prediction-component-the-visible-predicted-correspondences-are-weighted-in-the-correspondence-weighting-component-finally-they-are-back-projected-into-3d-and-feed-into-the-weighted-procrustes-aligner-which-estimates-the-relative-pose&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Components of the network&#34; srcset=&#34;
               /project/gr/components_hu15904271227194171968.webp 400w,
               /project/gr/components_hu10364728518287782599.webp 760w,
               /project/gr/components_hu16217144426837872915.webp 1200w&#34;
               src=&#34;https://marcb.pro/project/gr/components_hu15904271227194171968.webp&#34;
               width=&#34;581&#34;
               height=&#34;372&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Pipeline of our method. Given a pair of RGB-D images, $I1, D1 I2, D2$, we estimate the relative pose between these frames as $R \in SO(3)$ and $t \in R^3$ . First, $I1, I2$ are fed into the Correspondence and visibility prediction component, the visible predicted correspondences are weighted in the Correspondence Weighting component. Finally, they are back-projected into 3D and feed into the Weighted Procrustes aligner which estimates the relative pose.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
