<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>3D Reconstruction | Marc B</title>
    <link>https://marcb.pro/tag/3d-reconstruction/</link>
      <atom:link href="https://marcb.pro/tag/3d-reconstruction/index.xml" rel="self" type="application/rss+xml" />
    <description>3D Reconstruction</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Sat, 27 Mar 2021 17:56:52 +0100</lastBuildDate>
    <image>
      <url>https://marcb.pro/media/icon_hu7278467068447599620.png</url>
      <title>3D Reconstruction</title>
      <link>https://marcb.pro/tag/3d-reconstruction/</link>
    </image>
    
    <item>
      <title>Kinect Fusion: Dense Surface Mapping and Tracking</title>
      <link>https://marcb.pro/project/kinect-fusion/</link>
      <pubDate>Sat, 27 Mar 2021 17:56:52 +0100</pubDate>
      <guid>https://marcb.pro/project/kinect-fusion/</guid>
      <description>&lt;p&gt;Implementation of the paper &amp;ldquo;Kinect Fusion&amp;rdquo; by Newcombe et al. &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;  They presented a method for accurate real-time mapping of indoor scenes, using only a low-cost depth camera and graphics hardware.&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/3YIve6ju6qg?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;This project is part of the lecture 3D Scanning and Motion Capture (&lt;a href=&#34;https://www.in.tum.de/cg/teaching/winter-term-2021/3d-scanning-motion-capture/%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.in.tum.de/cg/teaching/winter-term-2021/3d-scanning-motion-capture/)&lt;/a&gt;. We implemented this project using C++ and CUDA.&lt;/p&gt;
&lt;h1 id=&#34;results&#34;&gt;Results&lt;/h1&gt;
&lt;p&gt;For evaluating our implementation we used TUM&amp;rsquo;s RGB-D SLAM Dataset
&lt;a href=&#34;https://vision.in.tum.de/data/datasets/rgbd-dataset&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://vision.in.tum.de/data/datasets/rgbd-dataset&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;See the final part of the video above to see the results.&lt;/p&gt;
&lt;h1 id=&#34;notes&#34;&gt;Notes&lt;/h1&gt;
&lt;!-- 

















&lt;figure  id=&#34;figure-a-caption&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;A caption&#34; srcset=&#34;
               /project/kinect-fusion/sidebyside_hu6977214877126391275.webp 400w,
               /project/kinect-fusion/sidebyside_hu10801717048972366410.webp 760w,
               /project/kinect-fusion/sidebyside_hu3483564292537808377.webp 1200w&#34;
               src=&#34;https://marcb.pro/project/kinect-fusion/sidebyside_hu6977214877126391275.webp&#34;
               width=&#34;760&#34;
               height=&#34;414&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      A caption
    &lt;/figcaption&gt;&lt;/figure&gt;
 --&gt;
&lt;!-- 

















&lt;figure  id=&#34;figure-a-caption&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;A caption&#34; srcset=&#34;
               /project/kinect-fusion/snap_hu11017619812430102650.webp 400w,
               /project/kinect-fusion/snap_hu6059672321614514695.webp 760w,
               /project/kinect-fusion/snap_hu17073304798016913143.webp 1200w&#34;
               src=&#34;https://marcb.pro/project/kinect-fusion/snap_hu11017619812430102650.webp&#34;
               width=&#34;760&#34;
               height=&#34;320&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      A caption
    &lt;/figcaption&gt;&lt;/figure&gt;
 --&gt;
&lt;!-- 

















&lt;figure  id=&#34;figure-a-caption&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;A caption&#34;
           src=&#34;https://marcb.pro/project/kinect-fusion/pipeline.svg&#34;
           loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      A caption
    &lt;/figcaption&gt;&lt;/figure&gt;
 --&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ismar2011.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ismar2011.pdf&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>SLAM for autonomous vehicles</title>
      <link>https://marcb.pro/project/warp/</link>
      <pubDate>Tue, 15 Sep 2020 20:11:42 +0100</pubDate>
      <guid>https://marcb.pro/project/warp/</guid>
      <description>&lt;p&gt;In this project, I worked on the SLAM pipeline for an autonomous driving vehicle.&lt;/p&gt;
&lt;p&gt;The tools I used for this project are, &lt;a href=&#34;https://ros.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ROS&lt;/a&gt;, C++, &lt;a href=&#34;https://pointclouds.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PCL library&lt;/a&gt;, &lt;a href=&#34;http://ceres-solver.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ceres Solver&lt;/a&gt;, and &lt;a href=&#34;https://google-cartographer.readthedocs.io/en/latest/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Cartographer&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In addition, I created a ROS package that generates a cost map from a point cloud generated by SLAM. A cost map is a matrix where each cell contains a cost value. This value expresses how likely it is that the cell is occupied by an object.
The cost value is also used to express &amp;ldquo;preferred&amp;rdquo; surfaces: for example, asphalt is preferred over grass.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Cost map&#34; srcset=&#34;
               /project/warp/costmap_hu2878952910471070744.webp 400w,
               /project/warp/costmap_hu4843652453929555716.webp 760w,
               /project/warp/costmap_hu7892964167120711589.webp 1200w&#34;
               src=&#34;https://marcb.pro/project/warp/costmap_hu2878952910471070744.webp&#34;
               width=&#34;760&#34;
               height=&#34;218&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;The images used belong to &lt;a href=&#34;https://warp.company/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://warp.company/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
