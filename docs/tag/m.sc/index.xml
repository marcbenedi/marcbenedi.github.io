<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>M.Sc | Marc B</title>
    <link>https://marcb.pro/tag/m.sc/</link>
      <atom:link href="https://marcb.pro/tag/m.sc/index.xml" rel="self" type="application/rss+xml" />
    <description>M.Sc</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Mon, 01 Nov 2021 00:00:00 +0100</lastBuildDate>
    <image>
      <url>https://marcb.pro/media/icon_hu7278467068447599620.png</url>
      <title>M.Sc</title>
      <link>https://marcb.pro/tag/m.sc/</link>
    </image>
    
    <item>
      <title>Learning Correspondences For Relative Pose Estimation</title>
      <link>https://marcb.pro/project/gr/</link>
      <pubDate>Mon, 01 Nov 2021 00:00:00 +0100</pubDate>
      <guid>https://marcb.pro/project/gr/</guid>
      <description>&lt;p&gt;We present an end-to-end learnable, differentiable method for pairwise relative pose registration of RGB-D frames. Our method is robust to big camera motions thanks to a self-supervised weighting of the predicted correspondences between the frames. Given a pair of frames, our method estimates matches of points and their visibility score. A self-supervised model predicts a confidence weight for visible matches. Finally, visible matches and their weight are fed into a differentiable weighted Procrustes aligner which estimates the rigid transformation between the input frames.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-pipeline-of-our-method-given-a-pair-of-rgb-d-images-i1-d1-i2-d2-we-estimate-the-relative-pose-between-these-frames-as-r-in-so3-and-t-in-r3--first-i1-i2-are-fed-into-the-correspondence-and-visibility-prediction-component-the-visible-predicted-correspondences-are-weighted-in-the-correspondence-weighting-component-finally-they-are-back-projected-into-3d-and-feed-into-the-weighted-procrustes-aligner-which-estimates-the-relative-pose&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Components of the network&#34; srcset=&#34;
               /project/gr/components_hu15904271227194171968.webp 400w,
               /project/gr/components_hu10364728518287782599.webp 760w,
               /project/gr/components_hu16217144426837872915.webp 1200w&#34;
               src=&#34;https://marcb.pro/project/gr/components_hu15904271227194171968.webp&#34;
               width=&#34;581&#34;
               height=&#34;372&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Pipeline of our method. Given a pair of RGB-D images, $I1, D1 I2, D2$, we estimate the relative pose between these frames as $R \in SO(3)$ and $t \in R^3$ . First, $I1, I2$ are fed into the Correspondence and visibility prediction component, the visible predicted correspondences are weighted in the Correspondence Weighting component. Finally, they are back-projected into 3D and feed into the Weighted Procrustes aligner which estimates the relative pose.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kinect Fusion: Dense Surface Mapping and Tracking</title>
      <link>https://marcb.pro/project/kinect-fusion/</link>
      <pubDate>Sat, 27 Mar 2021 17:56:52 +0100</pubDate>
      <guid>https://marcb.pro/project/kinect-fusion/</guid>
      <description>&lt;p&gt;Implementation of the paper &amp;ldquo;Kinect Fusion&amp;rdquo; by Newcombe et al. &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;  They presented a method for accurate real-time mapping of indoor scenes, using only a low-cost depth camera and graphics hardware.&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/3YIve6ju6qg?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;This project is part of the lecture 3D Scanning and Motion Capture (&lt;a href=&#34;https://www.in.tum.de/cg/teaching/winter-term-2021/3d-scanning-motion-capture/%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.in.tum.de/cg/teaching/winter-term-2021/3d-scanning-motion-capture/)&lt;/a&gt;. We implemented this project using C++ and CUDA.&lt;/p&gt;
&lt;h1 id=&#34;results&#34;&gt;Results&lt;/h1&gt;
&lt;p&gt;For evaluating our implementation we used TUM&amp;rsquo;s RGB-D SLAM Dataset
&lt;a href=&#34;https://vision.in.tum.de/data/datasets/rgbd-dataset&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://vision.in.tum.de/data/datasets/rgbd-dataset&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;See the final part of the video above to see the results.&lt;/p&gt;
&lt;h1 id=&#34;notes&#34;&gt;Notes&lt;/h1&gt;
&lt;!-- 

















&lt;figure  id=&#34;figure-a-caption&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;A caption&#34; srcset=&#34;
               /project/kinect-fusion/sidebyside_hu6977214877126391275.webp 400w,
               /project/kinect-fusion/sidebyside_hu10801717048972366410.webp 760w,
               /project/kinect-fusion/sidebyside_hu3483564292537808377.webp 1200w&#34;
               src=&#34;https://marcb.pro/project/kinect-fusion/sidebyside_hu6977214877126391275.webp&#34;
               width=&#34;760&#34;
               height=&#34;414&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      A caption
    &lt;/figcaption&gt;&lt;/figure&gt;
 --&gt;
&lt;!-- 

















&lt;figure  id=&#34;figure-a-caption&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;A caption&#34; srcset=&#34;
               /project/kinect-fusion/snap_hu11017619812430102650.webp 400w,
               /project/kinect-fusion/snap_hu6059672321614514695.webp 760w,
               /project/kinect-fusion/snap_hu17073304798016913143.webp 1200w&#34;
               src=&#34;https://marcb.pro/project/kinect-fusion/snap_hu11017619812430102650.webp&#34;
               width=&#34;760&#34;
               height=&#34;320&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      A caption
    &lt;/figcaption&gt;&lt;/figure&gt;
 --&gt;
&lt;!-- 

















&lt;figure  id=&#34;figure-a-caption&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;A caption&#34;
           src=&#34;https://marcb.pro/project/kinect-fusion/pipeline.svg&#34;
           loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      A caption
    &lt;/figcaption&gt;&lt;/figure&gt;
 --&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ismar2011.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ismar2011.pdf&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>SLAM for autonomous vehicles</title>
      <link>https://marcb.pro/project/warp/</link>
      <pubDate>Tue, 15 Sep 2020 20:11:42 +0100</pubDate>
      <guid>https://marcb.pro/project/warp/</guid>
      <description>&lt;p&gt;In this project, I worked on the SLAM pipeline for an autonomous driving vehicle.&lt;/p&gt;
&lt;p&gt;The tools I used for this project are, &lt;a href=&#34;https://ros.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ROS&lt;/a&gt;, C++, &lt;a href=&#34;https://pointclouds.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PCL library&lt;/a&gt;, &lt;a href=&#34;http://ceres-solver.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ceres Solver&lt;/a&gt;, and &lt;a href=&#34;https://google-cartographer.readthedocs.io/en/latest/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Cartographer&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In addition, I created a ROS package that generates a cost map from a point cloud generated by SLAM. A cost map is a matrix where each cell contains a cost value. This value expresses how likely it is that the cell is occupied by an object.
The cost value is also used to express &amp;ldquo;preferred&amp;rdquo; surfaces: for example, asphalt is preferred over grass.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Cost map&#34; srcset=&#34;
               /project/warp/costmap_hu2878952910471070744.webp 400w,
               /project/warp/costmap_hu4843652453929555716.webp 760w,
               /project/warp/costmap_hu7892964167120711589.webp 1200w&#34;
               src=&#34;https://marcb.pro/project/warp/costmap_hu2878952910471070744.webp&#34;
               width=&#34;760&#34;
               height=&#34;218&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;The images used belong to &lt;a href=&#34;https://warp.company/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://warp.company/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Divergence-Free Shape Correspondence with Time Dependent Vector Fields</title>
      <link>https://marcb.pro/project/idp/</link>
      <pubDate>Mon, 27 Apr 2020 20:09:10 +0100</pubDate>
      <guid>https://marcb.pro/project/idp/</guid>
      <description>&lt;p&gt;In this project, we extended the work of Eisenberger, Zorah, Cremers, &amp;ldquo;Divergence-Free Shape Interpolation and Correspondence&amp;rdquo; &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. In their work, they present a method to calculate deformation fields between shapes embedded in $\mathbb{R}^D$. To do so, they compute a divergence-free deformation field represented in a coarse-to-fine basis using the Karhunen-Loéve expansion.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-example-of-parts-of-a-shape-moving-in-different-directions-in-the-same-part-of-the-space-1httpsarxivorgabs180610417&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Example of parts of the shape moving in different directions at the same point&#34; srcset=&#34;
               /project/idp/original_failure_hu13969769669618843540.webp 400w,
               /project/idp/original_failure_hu10032394287242481926.webp 760w,
               /project/idp/original_failure_hu4949444032133230120.webp 1200w&#34;
               src=&#34;https://marcb.pro/project/idp/original_failure_hu13969769669618843540.webp&#34;
               width=&#34;464&#34;
               height=&#34;243&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Example of parts of a shape moving in different directions in the same part of the space &lt;a href=&#34;https://arxiv.org/abs/1806.10417&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[1]&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h1 id=&#34;our-contribution&#34;&gt;Our Contribution&lt;/h1&gt;
&lt;p&gt;As stated by the original authors, one of the limitations of their work is in movements where different parts of the shape move through the same
region of the embedding space in a contradictory manner.
Their method cannot model such deformation because a vector field can only contain a single vector per point in space.&lt;/p&gt;
&lt;p&gt;To overcome this limitation, Eisenberger suggested using time-dependent vector fields, such that a vector at a given point in space can change over time. For that, we solve correspondence and matching problems for the whole sequence during optimization.&lt;/p&gt;
&lt;h1 id=&#34;results&#34;&gt;Results&lt;/h1&gt;
&lt;p&gt;

    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/3cAmUDDTHYs?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;



    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/gUWHj9HRdkA?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;



    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/VISp8TJIm5g?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;
&lt;/p&gt;
&lt;h1 id=&#34;notes&#34;&gt;Notes&lt;/h1&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1806.10417&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/1806.10417&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
