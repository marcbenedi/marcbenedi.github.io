<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Marc B</title>
    <link>https://marcb.pro/project/</link>
      <atom:link href="https://marcb.pro/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Fri, 06 Jan 2023 17:36:39 +0100</lastBuildDate>
    <image>
      <url>https://marcb.pro/media/icon_hu70342a21c76a6b108e6681806083fa2b_864_512x512_fill_lanczos_center_3.png</url>
      <title>Projects</title>
      <link>https://marcb.pro/project/</link>
    </image>
    
    <item>
      <title>Learning Correspondences For Relative Pose Estimation</title>
      <link>https://marcb.pro/project/gr/</link>
      <pubDate>Mon, 01 Nov 2021 00:00:00 +0100</pubDate>
      <guid>https://marcb.pro/project/gr/</guid>
      <description>&lt;p&gt;We present an end-to-end learnable, differentiable method for pairwise relative pose registration of RGB-D frames. Our method is robust to big camera motions thanks to a self-supervised weighting of the predicted correspondences between the frames. Given a pair of frames, our method estimates matches of points and their visibility score. A self-supervised model predicts a confidence weight for visible matches. Finally, visible matches and their weight are fed into a differentiable weighted Procrustes aligner which estimates the rigid transformation between the input frames.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-pipeline-of-our-method-given-a-pair-of-rgb-d-images-i1-d1-i2-d2-we-estimate-the-relative-pose-between-these-frames-as-r-in-so3-and-t-in-r3--first-i1-i2-are-fed-into-the-correspondence-and-visibility-prediction-component-the-visible-predicted-correspondences-are-weighted-in-the-correspondence-weighting-component-finally-they-are-back-projected-into-3d-and-feed-into-the-weighted-procrustes-aligner-which-estimates-the-relative-pose&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Components of the network&#34; srcset=&#34;
               /project/gr/components_hu72fee0d4c4ad89edfe762f1a6a0f9247_155348_4ec1a5759ecd3cd05be370093352a715.webp 400w,
               /project/gr/components_hu72fee0d4c4ad89edfe762f1a6a0f9247_155348_1c6cf721cf8606b40fd2abad0941feff.webp 760w,
               /project/gr/components_hu72fee0d4c4ad89edfe762f1a6a0f9247_155348_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://marcb.pro/project/gr/components_hu72fee0d4c4ad89edfe762f1a6a0f9247_155348_4ec1a5759ecd3cd05be370093352a715.webp&#34;
               width=&#34;581&#34;
               height=&#34;372&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Pipeline of our method. Given a pair of RGB-D images, $I1, D1 I2, D2$, we estimate the relative pose between these frames as $R \in SO(3)$ and $t \in R^3$ . First, $I1, I2$ are fed into the Correspondence and visibility prediction component, the visible predicted correspondences are weighted in the Correspondence Weighting component. Finally, they are back-projected into 3D and feed into the Weighted Procrustes aligner which estimates the relative pose.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kinect Fusion: Dense Surface Mapping and Tracking</title>
      <link>https://marcb.pro/project/kinect-fusion/</link>
      <pubDate>Sat, 27 Mar 2021 17:56:52 +0100</pubDate>
      <guid>https://marcb.pro/project/kinect-fusion/</guid>
      <description>&lt;p&gt;Implementation of the paper &amp;ldquo;Kinect Fusion&amp;rdquo; by Newcombe et al. &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;  They presented a method for accurate real-time mapping of indoor scenes, using only a low-cost depth camera and graphics hardware.&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/3YIve6ju6qg?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;This project is part of the lecture 3D Scanning and Motion Capture (&lt;a href=&#34;https://www.in.tum.de/cg/teaching/winter-term-2021/3d-scanning-motion-capture/%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.in.tum.de/cg/teaching/winter-term-2021/3d-scanning-motion-capture/)&lt;/a&gt;. We implemented this project using C++ and CUDA.&lt;/p&gt;
&lt;h1 id=&#34;results&#34;&gt;Results&lt;/h1&gt;
&lt;p&gt;For evaluating our implementation we used TUM&amp;rsquo;s RGB-D SLAM Dataset
&lt;a href=&#34;https://vision.in.tum.de/data/datasets/rgbd-dataset&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://vision.in.tum.de/data/datasets/rgbd-dataset&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;See the final part of the video above to see the results.&lt;/p&gt;
&lt;h1 id=&#34;notes&#34;&gt;Notes&lt;/h1&gt;
&lt;!-- 

















&lt;figure  id=&#34;figure-a-caption&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;A caption&#34; srcset=&#34;
               /project/kinect-fusion/sidebyside_hucf10c783d6407f1a6f352ff6166590d6_121387_c8414a75cc17b5333eebcb8d35713c33.webp 400w,
               /project/kinect-fusion/sidebyside_hucf10c783d6407f1a6f352ff6166590d6_121387_43461ef68a3f6f9ffec15c5990c83ced.webp 760w,
               /project/kinect-fusion/sidebyside_hucf10c783d6407f1a6f352ff6166590d6_121387_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://marcb.pro/project/kinect-fusion/sidebyside_hucf10c783d6407f1a6f352ff6166590d6_121387_c8414a75cc17b5333eebcb8d35713c33.webp&#34;
               width=&#34;760&#34;
               height=&#34;414&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      A caption
    &lt;/figcaption&gt;&lt;/figure&gt;
 --&gt;
&lt;!-- 

















&lt;figure  id=&#34;figure-a-caption&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;A caption&#34; srcset=&#34;
               /project/kinect-fusion/snap_hu7142ece359247743daf3f145f6ebb63f_798400_728344d73080c84741ddf9d4b35f37ef.webp 400w,
               /project/kinect-fusion/snap_hu7142ece359247743daf3f145f6ebb63f_798400_090aebf261534b9bfbe1aaee1facf9fb.webp 760w,
               /project/kinect-fusion/snap_hu7142ece359247743daf3f145f6ebb63f_798400_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://marcb.pro/project/kinect-fusion/snap_hu7142ece359247743daf3f145f6ebb63f_798400_728344d73080c84741ddf9d4b35f37ef.webp&#34;
               width=&#34;760&#34;
               height=&#34;320&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      A caption
    &lt;/figcaption&gt;&lt;/figure&gt;
 --&gt;
&lt;!-- 

















&lt;figure  id=&#34;figure-a-caption&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;A caption&#34;
           src=&#34;https://marcb.pro/project/kinect-fusion/pipeline.svg&#34;
           loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      A caption
    &lt;/figcaption&gt;&lt;/figure&gt;
 --&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ismar2011.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ismar2011.pdf&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>SLAM for autonomous vehicles</title>
      <link>https://marcb.pro/project/warp/</link>
      <pubDate>Tue, 15 Sep 2020 20:11:42 +0100</pubDate>
      <guid>https://marcb.pro/project/warp/</guid>
      <description>&lt;p&gt;In this project, I worked on the SLAM pipeline for an autonomous driving vehicle.&lt;/p&gt;
&lt;p&gt;The tools I used for this project are, &lt;a href=&#34;https://ros.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ROS&lt;/a&gt;, C++, &lt;a href=&#34;https://pointclouds.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PCL library&lt;/a&gt;, &lt;a href=&#34;http://ceres-solver.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ceres Solver&lt;/a&gt;, and &lt;a href=&#34;https://google-cartographer.readthedocs.io/en/latest/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Cartographer&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In addition, I created a ROS package that generates a cost map from a point cloud generated by SLAM. A cost map is a matrix where each cell contains a cost value. This value expresses how likely it is that the cell is occupied by an object.
The cost value is also used to express &amp;ldquo;preferred&amp;rdquo; surfaces: for example, asphalt is preferred over grass.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Cost map&#34; srcset=&#34;
               /project/warp/costmap_hu44627bc17488b6d9c6aff366b1bca238_157606_b8263f9efdbd1ed5ea31391c0f968e49.webp 400w,
               /project/warp/costmap_hu44627bc17488b6d9c6aff366b1bca238_157606_f7cc1ed98c4ee135d19d059f3d8c1bf6.webp 760w,
               /project/warp/costmap_hu44627bc17488b6d9c6aff366b1bca238_157606_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://marcb.pro/project/warp/costmap_hu44627bc17488b6d9c6aff366b1bca238_157606_b8263f9efdbd1ed5ea31391c0f968e49.webp&#34;
               width=&#34;760&#34;
               height=&#34;218&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;The images used belong to &lt;a href=&#34;https://warp.company/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://warp.company/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Divergence-Free Shape Correspondence with Time Dependent Vector Fields</title>
      <link>https://marcb.pro/project/idp/</link>
      <pubDate>Mon, 27 Apr 2020 20:09:10 +0100</pubDate>
      <guid>https://marcb.pro/project/idp/</guid>
      <description>&lt;p&gt;In this project, we extended the work of Eisenberger, Zorah, Cremers, &amp;ldquo;Divergence-Free Shape Interpolation and Correspondence&amp;rdquo; &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. In their work, they present a method to calculate deformation fields between shapes embedded in $\mathbb{R}^D$. To do so, they compute a divergence-free deformation field represented in a coarse-to-fine basis using the Karhunen-Loéve expansion.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-example-of-parts-of-a-shape-moving-in-different-directions-in-the-same-part-of-the-space-1httpsarxivorgabs180610417&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Example of parts of the shape moving in different directions at the same point&#34; srcset=&#34;
               /project/idp/original_failure_hu2e417c26d152ffe032d80c0ee68ef5f2_11630_421cd020c588a96aa87c5d544d9acda3.webp 400w,
               /project/idp/original_failure_hu2e417c26d152ffe032d80c0ee68ef5f2_11630_8c9fbf702a2a7fa2927a5f8275b65aad.webp 760w,
               /project/idp/original_failure_hu2e417c26d152ffe032d80c0ee68ef5f2_11630_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://marcb.pro/project/idp/original_failure_hu2e417c26d152ffe032d80c0ee68ef5f2_11630_421cd020c588a96aa87c5d544d9acda3.webp&#34;
               width=&#34;464&#34;
               height=&#34;243&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Example of parts of a shape moving in different directions in the same part of the space &lt;a href=&#34;https://arxiv.org/abs/1806.10417&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[1]&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h1 id=&#34;our-contribution&#34;&gt;Our Contribution&lt;/h1&gt;
&lt;p&gt;As stated by the original authors, one of the limitations of their work is in movements where different parts of the shape move through the same
region of the embedding space in a contradictory manner.
Their method cannot model such deformation because a vector field can only contain a single vector per point in space.&lt;/p&gt;
&lt;p&gt;To overcome this limitation, Eisenberger suggested using time-dependent vector fields, such that a vector at a given point in space can change over time. For that, we solve correspondence and matching problems for the whole sequence during optimization.&lt;/p&gt;
&lt;h1 id=&#34;results&#34;&gt;Results&lt;/h1&gt;
&lt;p&gt;

    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/3cAmUDDTHYs?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;



    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/gUWHj9HRdkA?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;



    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/VISp8TJIm5g?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;
&lt;/p&gt;
&lt;h1 id=&#34;notes&#34;&gt;Notes&lt;/h1&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1806.10417&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/1806.10417&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Design of an environment for solving pseudo-Boolean optimization problems</title>
      <link>https://marcb.pro/project/bachelors-thesis/</link>
      <pubDate>Tue, 26 Jun 2018 20:05:18 +0100</pubDate>
      <guid>https://marcb.pro/project/bachelors-thesis/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Bachelor&amp;rsquo;s thesis&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Boolean_satisfiability_problem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Boolean Satisfiability problems (SAT)&lt;/a&gt; consists of finding a valid assignment (model) for a set of Boolean variables. It was the first problem proven to be NP-Complete which allowed reducing many NP-Complete problems to it. Because of this, it is one of the pillars of Computer Science.&lt;/p&gt;
&lt;p&gt;An extension to SAT is Pseud-Boolean optimization problems. Unlike SAT, which can only express binary relationships among variables, Pseudo-Boolean optimization can formalize more &lt;a href=&#34;https://en.wikipedia.org/wiki/Pseudo-Boolean_function&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;complex relationships&lt;/a&gt;. These problems are defined in the following manner:&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-pseudo-boolean-optimization-formulation-figure-source-mpbo-a-distributed-pseudo-boolean-optimization-solver-1httpswwwsemanticscholarorgpapermpbo-a-distributed-pseudo-boolean-optimization-santos-godinho52ee5d8996c6f6d5caa43ad35155f404ddc584e3&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;TODO&#34; srcset=&#34;
               /project/bachelors-thesis/pb_formula_hua71ef25e4bba6659a85d75e59ef303b2_12709_e5cb62b5301550b5c2dd1c0c98f4f670.webp 400w,
               /project/bachelors-thesis/pb_formula_hua71ef25e4bba6659a85d75e59ef303b2_12709_06740f99715093b9606e778a6b357731.webp 760w,
               /project/bachelors-thesis/pb_formula_hua71ef25e4bba6659a85d75e59ef303b2_12709_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://marcb.pro/project/bachelors-thesis/pb_formula_hua71ef25e4bba6659a85d75e59ef303b2_12709_e5cb62b5301550b5c2dd1c0c98f4f670.webp&#34;
               width=&#34;400&#34;
               height=&#34;272&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Pseudo-Boolean Optimization formulation. Figure source: MPBO A Distributed Pseudo-Boolean Optimization Solver &lt;a href=&#34;https://www.semanticscholar.org/paper/MPBO-A-Distributed-Pseudo-Boolean-Optimization-Santos-Godinho/52ee5d8996c6f6d5caa43ad35155f404ddc584e3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[1]&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h1 id=&#34;example-knapsack-problem&#34;&gt;Example: Knapsack problem&lt;/h1&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-source-2httpsenwikipediaorgwikiknapsack_problem&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;TODO&#34; srcset=&#34;
               /project/bachelors-thesis/250px-Knapsack.svg_hu52b2a063a60a593cdca3ed71e3480a80_25397_668a4392834c3bb7560435be8634fae9.webp 400w,
               /project/bachelors-thesis/250px-Knapsack.svg_hu52b2a063a60a593cdca3ed71e3480a80_25397_3eb7af30ad94364b64123bd36a23de38.webp 760w,
               /project/bachelors-thesis/250px-Knapsack.svg_hu52b2a063a60a593cdca3ed71e3480a80_25397_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://marcb.pro/project/bachelors-thesis/250px-Knapsack.svg_hu52b2a063a60a593cdca3ed71e3480a80_25397_668a4392834c3bb7560435be8634fae9.webp&#34;
               width=&#34;250&#34;
               height=&#34;217&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Source &lt;a href=&#34;https://en.wikipedia.org/wiki/Knapsack_problem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[2]&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Given a set of items, each one with a value and a weight, and a knapsack with a maximum capacity, the goal is to select some objects to put inside the knapsack in a way that the weight is not bigger than the knapsack’s capacity and the total value is maximised.&lt;/p&gt;
&lt;p&gt;The variables for this problem, given $n$ objects, are:
$$o_1, o_2, \ldots , o_n \text{for the objects}$$
$$w_1, w_2, \ldots , w_n \text{for the weights}$$
$$v_1, v_2, \ldots , v_n \text{for the values}$$
There is only one constraint which represent the maximum capacity of the knapsack problem:
$$w_0 o_0 + w_1 o_1+  \ldots w_n o_n \leq knapsack&amp;rsquo;s \ capacity$$
And the cost function:
$$v_0 o_0 + v_1 o_1+  \ldots v_n o_n $$&lt;/p&gt;
&lt;h1 id=&#34;sat-solvers&#34;&gt;SAT Solvers&lt;/h1&gt;
&lt;p&gt;A SAT solver is a tool that aims at solving Boolean Satisfiability problems. However, they take as input Boolean formulas in Conjunctive Normal Form (CNF).
In this project, we propose an efficient encoding for converting Pseudo-Boolean minimisation problems into CNF constraints. If this encoding is not efficient, the size and difficulty of the problem can increase exponentially.&lt;/p&gt;
&lt;h1 id=&#34;proposed-encoding&#34;&gt;Proposed encoding&lt;/h1&gt;
&lt;p&gt;The proposed encoding is based on the idea that if a function&amp;rsquo;s primes cover a small space then a lot of clauses will be required when encoding it into a CNF.
For more details see the &lt;a href=&#34;https://marcb.pro/project/tfg/slides.pdf&#34;&gt;slides&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;notes&#34;&gt;Notes&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.semanticscholar.org/paper/MPBO-A-Distributed-Pseudo-Boolean-Optimization-Santos-Godinho/52ee5d8996c6f6d5caa43ad35155f404ddc584e3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.semanticscholar.org/paper/MPBO-A-Distributed-Pseudo-Boolean-Optimization-Santos-Godinho/52ee5d8996c6f6d5caa43ad35155f404ddc584e3&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Knapsack_problem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://en.wikipedia.org/wiki/Knapsack_problem&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>StirHack 2016</title>
      <link>https://marcb.pro/project/hackathon-stirhack-2016/</link>
      <pubDate>Fri, 12 Feb 2016 19:59:25 +0100</pubDate>
      <guid>https://marcb.pro/project/hackathon-stirhack-2016/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Writing about StirHack16, the first hackathon I ever attended, brings me very good memories. In this page I will focus on the project itself, although some day I should write about the hackathon experience. 😃&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Android application with Google Maps API interface to help hackathon participants. The app provides the following information: List of upcoming hackathons, their position on Google Maps, and points of interest such as restaurants and hotels around the event.&lt;/p&gt;
&lt;p&gt;For more information visit the &lt;a href=&#34;https://devpost.com/software/hackworld-gz18v7&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;project&amp;rsquo;s page on Devpost&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /project/hackathon-stirhack-2016/tweet_hud4067377e6f941c58964826ef00f110c_142754_cb96ac78de0f3def16eb438622ba6941.webp 400w,
               /project/hackathon-stirhack-2016/tweet_hud4067377e6f941c58964826ef00f110c_142754_c12bdbf40f9722c3c53976969874d4c1.webp 760w,
               /project/hackathon-stirhack-2016/tweet_hud4067377e6f941c58964826ef00f110c_142754_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://marcb.pro/project/hackathon-stirhack-2016/tweet_hud4067377e6f941c58964826ef00f110c_142754_cb96ac78de0f3def16eb438622ba6941.webp&#34;
               width=&#34;356&#34;
               height=&#34;508&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
