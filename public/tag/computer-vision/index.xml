<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Computer Vision | Marc Benedi</title>
    <link>https://marcb.pro/tag/computer-vision/</link>
      <atom:link href="https://marcb.pro/tag/computer-vision/index.xml" rel="self" type="application/rss+xml" />
    <description>Computer Vision</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Â© 2022 marcb.pro</copyright><lastBuildDate>Mon, 01 Nov 2021 00:00:00 +0100</lastBuildDate>
    <image>
      <url>https://marcb.pro/media/icon_hu70342a21c76a6b108e6681806083fa2b_864_512x512_fill_lanczos_center_3.png</url>
      <title>Computer Vision</title>
      <link>https://marcb.pro/tag/computer-vision/</link>
    </image>
    
    <item>
      <title>Learning Correspondences For Relative Pose Estimation</title>
      <link>https://marcb.pro/project/gr/</link>
      <pubDate>Mon, 01 Nov 2021 00:00:00 +0100</pubDate>
      <guid>https://marcb.pro/project/gr/</guid>
      <description>&lt;p&gt;We present an end-to-end learnable, differentiable method for pairwise relative pose registration of RGB-D frames. Our method is robust to big camera motions thanks to a self-supervised weighting of the predicted correspondences between the frames. Given a pair of frames, our method estimates matches of points and their visibility score. A self-supervised model predicts a confidence weight for visible matches. Finally, visible matches and their weight are fed into a differentiable weighted Procrustes aligner which estimates the rigid transformation between the input frames.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-pipeline-of-our-method-given-a-pair-of-rgb-d-images-i1-d1-i2-d2-we-estimate-the-relative-pose-between-these-frames-as-r-in-so3-and-t-in-r3--first-i1-i2-are-fed-into-the-correspondence-and-visibility-prediction-component-the-visible-predicted-correspondences-are-weighted-in-the-correspondence-weighting-component-finally-they-are-back-projected-into-3d-and-feed-into-the-weighted-procrustes-aligner-which-estimates-the-relative-pose&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Components of the network&#34; srcset=&#34;
               /project/gr/components_hu72fee0d4c4ad89edfe762f1a6a0f9247_155348_9299e91191ac3889b142e7340faac071.png 400w,
               /project/gr/components_hu72fee0d4c4ad89edfe762f1a6a0f9247_155348_6296029f151b3df3c2ccc25683dbbb91.png 760w,
               /project/gr/components_hu72fee0d4c4ad89edfe762f1a6a0f9247_155348_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://marcb.pro/project/gr/components_hu72fee0d4c4ad89edfe762f1a6a0f9247_155348_9299e91191ac3889b142e7340faac071.png&#34;
               width=&#34;581&#34;
               height=&#34;372&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Pipeline of our method. Given a pair of RGB-D images, $I1, D1 I2, D2$, we estimate the relative pose between these frames as $R \in SO(3)$ and $t \in R^3$ . First, $I1, I2$ are fed into the Correspondence and visibility prediction component, the visible predicted correspondences are weighted in the Correspondence Weighting component. Finally, they are back-projected into 3D and feed into the Weighted Procrustes aligner which estimates the relative pose.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kinect Fusion: Dense Surface Mapping and Tracking</title>
      <link>https://marcb.pro/project/kinect-fusion/</link>
      <pubDate>Sat, 27 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://marcb.pro/project/kinect-fusion/</guid>
      <description>&lt;p&gt;Implementation of the paper &amp;ldquo;Kinect Fusion&amp;rdquo; by Newcombe et al. &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;  They presented a method for accurate real-time mapping of indoor scenes, using only a low-cost depth camera and graphics hardware.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/3YIve6ju6qg&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;This project is part of the lecture 3D Scanning and Motion Capture (&lt;a href=&#34;https://www.in.tum.de/cg/teaching/winter-term-2021/3d-scanning-motion-capture/%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.in.tum.de/cg/teaching/winter-term-2021/3d-scanning-motion-capture/)&lt;/a&gt;. We implemented this project using C++ and CUDA.&lt;/p&gt;
&lt;h1 id=&#34;results&#34;&gt;Results&lt;/h1&gt;
&lt;p&gt;For evaluating our implementation we used TUM&amp;rsquo;s RGB-D SLAM Dataset
&lt;a href=&#34;https://vision.in.tum.de/data/datasets/rgbd-dataset&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://vision.in.tum.de/data/datasets/rgbd-dataset&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;See the final part of the video above to see the results.&lt;/p&gt;
&lt;h1 id=&#34;notes&#34;&gt;Notes&lt;/h1&gt;
&lt;!-- 













&lt;figure  id=&#34;figure-a-caption&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;A caption&#34; srcset=&#34;
               /project/kinect-fusion/sidebyside_hucf10c783d6407f1a6f352ff6166590d6_121387_7fb7384af53760184ec747fc9e3fdc70.png 400w,
               /project/kinect-fusion/sidebyside_hucf10c783d6407f1a6f352ff6166590d6_121387_b6e53d3fd854c257a46bec0238f8ca60.png 760w,
               /project/kinect-fusion/sidebyside_hucf10c783d6407f1a6f352ff6166590d6_121387_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://marcb.pro/project/kinect-fusion/sidebyside_hucf10c783d6407f1a6f352ff6166590d6_121387_7fb7384af53760184ec747fc9e3fdc70.png&#34;
               width=&#34;760&#34;
               height=&#34;414&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      A caption
    &lt;/figcaption&gt;&lt;/figure&gt; --&gt;
&lt;!-- 













&lt;figure  id=&#34;figure-a-caption&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;A caption&#34; srcset=&#34;
               /project/kinect-fusion/snap_hu7142ece359247743daf3f145f6ebb63f_798400_92198bcbc200457d81eef0ddf889de52.png 400w,
               /project/kinect-fusion/snap_hu7142ece359247743daf3f145f6ebb63f_798400_23ca2ba8e2ace22e8af0de181fe249ad.png 760w,
               /project/kinect-fusion/snap_hu7142ece359247743daf3f145f6ebb63f_798400_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://marcb.pro/project/kinect-fusion/snap_hu7142ece359247743daf3f145f6ebb63f_798400_92198bcbc200457d81eef0ddf889de52.png&#34;
               width=&#34;760&#34;
               height=&#34;320&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      A caption
    &lt;/figcaption&gt;&lt;/figure&gt; --&gt;
&lt;!-- 













&lt;figure  id=&#34;figure-a-caption&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;A caption&#34;
           src=&#34;https://marcb.pro/project/kinect-fusion/pipeline.svg&#34;
           loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      A caption
    &lt;/figcaption&gt;&lt;/figure&gt; --&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ismar2011.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ismar2011.pdf&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
  </channel>
</rss>
